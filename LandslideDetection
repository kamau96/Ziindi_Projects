{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11474913,"sourceType":"datasetVersion","datasetId":7191644}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Landslide Detection \nThe goal of this notebook is to build a model capable of classifying satellite imagery to identify the occurrence of landslides.","metadata":{}},{"cell_type":"code","source":"# Libraries used for the project\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nfrom pathlib import Path\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T14:01:16.732813Z","iopub.execute_input":"2025-07-28T14:01:16.733053Z","iopub.status.idle":"2025-07-28T14:01:30.822769Z","shell.execute_reply.started":"2025-07-28T14:01:16.733030Z","shell.execute_reply":"2025-07-28T14:01:30.821982Z"}},"outputs":[{"name":"stderr","text":"2025-07-28 14:01:19.415142: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753711279.609729      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753711279.667189      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Data Exploration","metadata":{}},{"cell_type":"code","source":"train_data_path = \"/kaggle/input/slideandseekclasificationlandslidedetectiondataset/train_data/train_data\"\ntrain_csv_path = \"/kaggle/input/slideandseekclasificationlandslidedetectiondataset/Train.csv\"\n\ntest_data_path = \"/kaggle/input/slideandseekclasificationlandslidedetectiondataset/test_data/test_data\"\ntest_csv_path = \"/kaggle/input/slideandseekclasificationlandslidedetectiondataset/Test.csv\"\n\ntrain_csv = pd.read_csv(train_csv_path)\ntest_csv= pd.read_csv(test_csv_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T14:01:30.823547Z","iopub.execute_input":"2025-07-28T14:01:30.824019Z","iopub.status.idle":"2025-07-28T14:01:30.852908Z","shell.execute_reply.started":"2025-07-28T14:01:30.823999Z","shell.execute_reply":"2025-07-28T14:01:30.852242Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def build_tf_dataset(npy_dir_path, label_df=None, batch_size=32, shuffle=True):\n    npy_dir = Path(npy_dir_path)\n\n    # Step 1: List all .npy files\n    file_paths = list(npy_dir.glob(\"*.npy\"))\n\n    # Step 2: If labels are available, map IDs to labels\n    if label_df is not None:\n        label_df[\"ID\"] = label_df[\"ID\"].astype(str)\n        label_map = dict(zip(label_df[\"ID\"], label_df[\"label\"]))\n\n        # Build (file_path, label) pairs\n        data = []\n        for file in file_paths:\n            file_id = file.stem  # e.g. '1234' from '1234.npy'\n            if file_id in label_map:\n                label = label_map[file_id]\n                data.append((str(file), label))\n\n        file_paths, labels = zip(*data)\n    else:\n        # Just raw file paths for unlabeled data (e.g. test set)\n        file_paths = [str(f) for f in file_paths]\n        labels = None\n\n    # Step 3: Define how to load one sample\n    def load_npy(file_path, label=None):\n        def _load(path):\n            arr = np.load(path.numpy().decode(\"utf-8\"))\n            return arr.astype(np.float32)\n\n        image = tf.py_function(_load, [file_path], tf.float32)\n        image.set_shape([64, 64, 12]) \n\n        if label is not None:\n            return image, label\n        else:\n            return image\n\n    # Step 4: Build tf.data.Dataset\n    if labels is not None:\n        ds = tf.data.Dataset.from_tensor_slices((list(file_paths), list(labels)))\n        ds = ds.map(load_npy, num_parallel_calls=tf.data.AUTOTUNE)\n    else:\n        ds = tf.data.Dataset.from_tensor_slices(list(file_paths))\n        ds = ds.map(lambda x: load_npy(x, None), num_parallel_calls=tf.data.AUTOTUNE)\n\n    if shuffle and labels is not None:\n        ds = ds.shuffle(buffer_size=len(file_paths))\n\n    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\n    return ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T14:01:30.854704Z","iopub.execute_input":"2025-07-28T14:01:30.854970Z","iopub.status.idle":"2025-07-28T14:01:30.880424Z","shell.execute_reply.started":"2025-07-28T14:01:30.854953Z","shell.execute_reply":"2025-07-28T14:01:30.879729Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"dataset = build_tf_dataset(train_data_path, train_csv)\ntest_dataset = build_tf_dataset(test_data_path)\n\ntotal_batches = sum(1 for _ in dataset)\ntrain_dataset = dataset.take(int(total_batches * 0.9))\nvalid_dataset = dataset.skip(int(total_batches * 0.9))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T14:01:30.881137Z","iopub.execute_input":"2025-07-28T14:01:30.881315Z","iopub.status.idle":"2025-07-28T14:01:51.405298Z","shell.execute_reply.started":"2025-07-28T14:01:30.881300Z","shell.execute_reply":"2025-07-28T14:01:51.404711Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1753711291.701224      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## Model Building","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}