{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11474913,"sourceType":"datasetVersion","datasetId":7191644}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## üõ∞Ô∏è Landslide Detection\n\nLandslides are rapid ground movements often triggered by natural forces such as heavy rainfall or seismic activity. Their impact can be devastating‚Äîleading to loss of life, destruction of property and infrastructure, and long-term environmental damage such as deforestation and soil erosion. In regions prone to these events, early detection and monitoring are critical to minimizing their impact and enabling timely response.\n\nThis notebook explores the use of satellite-based Earth observation data to detect landslide-affected areas. By leveraging multispectral and radar imagery, the goal is to build a robust machine learning model that can identify regions where landslides have occurred. Such a system could enhance disaster preparedness efforts and support real-time decision-making in vulnerable areas.\n","metadata":{}},{"cell_type":"markdown","source":"### üìö Libraries Used\n\nThe following Python libraries are utilized throughout this notebook to support data analysis, model development, and visualization:\n\n* **pandas** ‚Äì for loading, cleaning, and manipulating tabular datasets.\n* **numpy** ‚Äì for efficient numerical computations and array operations.\n* **Path** ‚Äì for working with the file system.\n* **tensorflow** ‚Äì for building and training the deep learning model used in landslide detection.\n* **scikit-learn** ‚Äì for preprocessing, model evaluation metrics, and classical machine learning utilities.\n* **seaborn** ‚Äì for enhanced statistical visualizations and data exploration.\n* **matplotlib** ‚Äì for creating detailed plots to visualize trends, distributions, and model performance.\n","metadata":{"execution":{"iopub.status.busy":"2025-08-04T19:41:44.444855Z","iopub.execute_input":"2025-08-04T19:41:44.445605Z","iopub.status.idle":"2025-08-04T19:41:44.454363Z","shell.execute_reply.started":"2025-08-04T19:41:44.445574Z","shell.execute_reply":"2025-08-04T19:41:44.453481Z"}}},{"cell_type":"code","source":"# Libraries\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\n\nfrom pathlib import Path\nfrom sklearn.metrics import f1_score, confusion_matrix\nfrom tensorflow import keras","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T20:37:55.352579Z","iopub.execute_input":"2025-08-04T20:37:55.352939Z","iopub.status.idle":"2025-08-04T20:37:55.358103Z","shell.execute_reply.started":"2025-08-04T20:37:55.352914Z","shell.execute_reply":"2025-08-04T20:37:55.357201Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### üóÇÔ∏è Data Access\n\nThe dataset used in this project is hosted on [Kaggle](https://www.kaggle.com), and it includes both **CSV files** (for labels and metadata) and **satellite images** relevant to landslide detection. To begin, we will first download and extract the dataset from Kaggle. The CSV files will provide structured information such as class labels, while the image files will serve as input data for model training and evaluation.\n","metadata":{}},{"cell_type":"code","source":"# Here is where the data is located\ntrain_data_path = \"/kaggle/input/slideandseekclasificationlandslidedetectiondataset/train_data/train_data\"\ntest_data_path = \"/kaggle/input/slideandseekclasificationlandslidedetectiondataset/test_data/test_data\"\n\ntrain_csv_path = \"/kaggle/input/slideandseekclasificationlandslidedetectiondataset/Train.csv\"\ntest_csv_path = \"/kaggle/input/slideandseekclasificationlandslidedetectiondataset/Test.csv\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T20:47:04.201706Z","iopub.execute_input":"2025-08-04T20:47:04.202012Z","iopub.status.idle":"2025-08-04T20:47:04.207072Z","shell.execute_reply.started":"2025-08-04T20:47:04.201990Z","shell.execute_reply":"2025-08-04T20:47:04.206146Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Data with image id and labels for train\ntrain_csv = pd.read_csv(train_csv_path)\ntest_csv = pd.read_csv(test_csv_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T20:47:05.180087Z","iopub.execute_input":"2025-08-04T20:47:05.180741Z","iopub.status.idle":"2025-08-04T20:47:05.229444Z","shell.execute_reply.started":"2025-08-04T20:47:05.180713Z","shell.execute_reply":"2025-08-04T20:47:05.228596Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def load_images(file_path, labels=None, num_images=None):\n    \"\"\"\n    Loads satellite image data stored as .npy files and returns them as a NumPy array.\n    \n    If label data is provided, the function also returns a corresponding array of labels.\n\n    Parameters:\n    ----------\n    file_path : str or Path\n        Path to the directory containing .npy image files (each with shape [64, 64, 12]).\n        \n    labels : DataFrame, optional\n        A Pandas DataFrame containing two columns: 'ID' (image filename stem) and 'label'.\n        Used to assign a label to each image based on filename matching.\n\n    num_images : int\n        The number of images to load. This is required to preallocate memory for the image and label arrays.\n\n    Returns:\n    -------\n    images_array : ndarray\n        A NumPy array of shape (num_images, 64, 64, 12) containing the loaded images.\n\n    labels_array : ndarray, optional\n        A NumPy array of shape (num_images,) containing labels corresponding to each image,\n        returned only if `labels` is provided.\n    \"\"\"\n    \n    # Generate an iterable of all .npy files in the given directory\n    images_generator = Path(file_path).glob(\"*.npy\")\n\n    # Preallocate image array\n    images_array = np.ones((num_images, 64, 64, 12), dtype=np.float32)\n\n    # Initialize labels array if label data is provided\n    if labels is not None:\n        label_mapping = dict(zip(labels[\"ID\"], labels[\"label\"]))\n        labels_array = np.ones(num_images, dtype=np.int8)\n\n    # Load each .npy image and its corresponding label (if available)\n    for i, image_path in enumerate(images_generator):\n        curr_image = np.load(image_path)\n        images_array[i] = curr_image\n\n        if labels is not None:\n            image_id = image_path.stem  # filename without extension\n            image_label = label_mapping[image_id]\n            labels_array[i] = image_label\n\n    if labels is not None:\n        return images_array, labels_array\n    return images_array\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T22:07:39.745912Z","iopub.execute_input":"2025-08-04T22:07:39.746227Z","iopub.status.idle":"2025-08-04T22:07:39.754801Z","shell.execute_reply.started":"2025-08-04T22:07:39.746199Z","shell.execute_reply":"2025-08-04T22:07:39.753792Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# Number of images\ntrain_images_length = train_csv[\"ID\"].nunique()\ntest_images_length = test_csv[\"ID\"].nunique()\n\n# Load images and labels\nX_train, y_train = load_images(train_data_path, train_csv, train_images_length)\nX_test = load_images(test_data_path, num_images=test_images_length)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T22:14:30.692839Z","iopub.execute_input":"2025-08-04T22:14:30.693469Z","iopub.status.idle":"2025-08-04T22:15:49.320360Z","shell.execute_reply.started":"2025-08-04T22:14:30.693444Z","shell.execute_reply":"2025-08-04T22:15:49.319616Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"### üîç Exploratory Data Analysis (EDA)\n\nWith the image data now loaded, we can begin exploring it in depth. This step is essential for gaining a deeper understanding of the dataset‚Äôs structure, distribution, and potential anomalies. A well-executed EDA not only guides preprocessing decisions but also helps reveal patterns that may influence model performance. In short, this is where we begin to \"become one with the data.\"","metadata":{}},{"cell_type":"code","source":"# Let's quickly look at min and max\nprint(f\"Train data max: {X_train.max():.2f}, min: {X_train.min():.2f}, and std: {X_train.std():.2f}\")\nprint(f\"Test data max: {X_test.max():.2f}, min: {X_test.min():.2f}, and std: {X_test.std():.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T22:30:30.990583Z","iopub.execute_input":"2025-08-04T22:30:30.990897Z","iopub.status.idle":"2025-08-04T22:30:33.189802Z","shell.execute_reply.started":"2025-08-04T22:30:30.990875Z","shell.execute_reply":"2025-08-04T22:30:33.189012Z"}},"outputs":[{"name":"stdout","text":"Train data max: 20944.00, min: -65.66, and std: 1329.00\nTest data max: 20016.00, min: -62.72, and std: 1333.33\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"The analysis above indicates that normalization is necessary for our image dataset. Bringing pixel values into a consistent range‚Äîspecifically between 0 and 1‚Äîwill help stabilize training, improve convergence, and ensure numerical compatibility with most deep learning models. Therefore, we will proceed to normalize all image bands to fall within this range.","metadata":{}},{"cell_type":"code","source":"def normalize_per_band(images):\n    \"\"\"\n    Normalize each spectral band independently across the dataset to [0, 1].\n    \n    Parameters:\n    -----------\n    images : np.ndarray\n        A NumPy array of shape (N, H, W, C).\n        \n    Returns:\n    --------\n    np.ndarray\n        Band-normalized image array.\n    \"\"\"\n    normalized = np.empty_like(images, dtype=np.float32)\n    for band in range(images.shape[-1]):\n        band_data = images[..., band]\n        band_min = band_data.min()\n        band_max = band_data.max()\n        normalized[..., band] = (band_data - band_min) / (band_max - band_min + 1e-5)\n    return normalized","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T22:52:30.907434Z","iopub.execute_input":"2025-08-04T22:52:30.908363Z","iopub.status.idle":"2025-08-04T22:52:30.913836Z","shell.execute_reply.started":"2025-08-04T22:52:30.908333Z","shell.execute_reply":"2025-08-04T22:52:30.912765Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"# Normalize the images\nX_train_normalized = normalize_per_band(X_train)\nX_test_normalized = normalize_per_band(X_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T22:52:31.759038Z","iopub.execute_input":"2025-08-04T22:52:31.759498Z","iopub.status.idle":"2025-08-04T22:52:44.176277Z","shell.execute_reply.started":"2025-08-04T22:52:31.759475Z","shell.execute_reply":"2025-08-04T22:52:44.175221Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"# Let's quickly look at normalized min and max\nprint(f\"Train data max: {X_train_normalized.max():.2f}, min: {X_train_normalized.min():.2f}, and std: {X_train_normalized.std():.2f}\")\nprint(f\"Test data max: {X_test_normalized.max():.2f}, min: {X_test_normalized.min():.2f}, and std: {X_test_normalized.std():.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-04T22:52:44.177640Z","iopub.execute_input":"2025-08-04T22:52:44.177929Z","iopub.status.idle":"2025-08-04T22:52:46.363297Z","shell.execute_reply.started":"2025-08-04T22:52:44.177908Z","shell.execute_reply":"2025-08-04T22:52:46.362330Z"}},"outputs":[{"name":"stdout","text":"Train data max: 1.00, min: 0.00, and std: 0.21\nTest data max: 1.00, min: 0.00, and std: 0.21\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}