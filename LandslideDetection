{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11474913,"sourceType":"datasetVersion","datasetId":7191644}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Landslide Detection \nThe goal of this notebook is to build a model capable of classifying satellite imagery to identify the occurrence of landslides.","metadata":{}},{"cell_type":"code","source":"# Libraries used for the project\nimport numpy as np\nimport os\nimport pandas as pd\nfrom pathlib import Path\nimport tensorflow as tf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T12:24:30.516744Z","iopub.execute_input":"2025-07-24T12:24:30.517347Z","iopub.status.idle":"2025-07-24T12:24:45.300262Z","shell.execute_reply.started":"2025-07-24T12:24:30.517323Z","shell.execute_reply":"2025-07-24T12:24:45.299686Z"}},"outputs":[{"name":"stderr","text":"2025-07-24 12:24:32.557192: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753359872.780982      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753359872.846730      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Data Exploration","metadata":{}},{"cell_type":"code","source":"train_data_path = \"/kaggle/input/slideandseekclasificationlandslidedetectiondataset/train_data/train_data\"\ntrain_csv_path = \"/kaggle/input/slideandseekclasificationlandslidedetectiondataset/Train.csv\"\n\ntest_data_path = \"/kaggle/input/slideandseekclasificationlandslidedetectiondataset/test_data/test_data\"\ntest_csv_path = \"/kaggle/input/slideandseekclasificationlandslidedetectiondataset/Test.csv\"\n\ntrain_csv = pd.read_csv(train_csv_path)\ntest_csv= pd.read_csv(test_csv_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T12:24:45.301311Z","iopub.execute_input":"2025-07-24T12:24:45.301737Z","iopub.status.idle":"2025-07-24T12:24:45.335189Z","shell.execute_reply.started":"2025-07-24T12:24:45.301718Z","shell.execute_reply":"2025-07-24T12:24:45.334486Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"train_csv.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T12:24:45.335935Z","iopub.execute_input":"2025-07-24T12:24:45.336111Z","iopub.status.idle":"2025-07-24T12:24:45.440037Z","shell.execute_reply.started":"2025-07-24T12:24:45.336095Z","shell.execute_reply":"2025-07-24T12:24:45.439221Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"          ID  label\n0  ID_HUD1ST      1\n1  ID_KGE2HY      1\n2  ID_VHV9BL      1\n3  ID_ZT0VEJ      0\n4  ID_5NFXVY      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID_HUD1ST</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID_KGE2HY</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID_VHV9BL</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID_ZT0VEJ</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID_5NFXVY</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"test_csv.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T12:24:45.441258Z","iopub.execute_input":"2025-07-24T12:24:45.441541Z","iopub.status.idle":"2025-07-24T12:24:45.448161Z","shell.execute_reply.started":"2025-07-24T12:24:45.441523Z","shell.execute_reply":"2025-07-24T12:24:45.447476Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"          ID\n0  ID_ICB8K9\n1  ID_2D4AOJ\n2  ID_2TVPI0\n3  ID_E05WIK\n4  ID_KKFDJO","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID_ICB8K9</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID_2D4AOJ</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID_2TVPI0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID_E05WIK</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID_KKFDJO</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"train_csv[\"ID\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T12:24:45.448776Z","iopub.execute_input":"2025-07-24T12:24:45.448941Z","iopub.status.idle":"2025-07-24T12:24:45.468158Z","shell.execute_reply.started":"2025-07-24T12:24:45.448927Z","shell.execute_reply":"2025-07-24T12:24:45.467639Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"0       ID_HUD1ST\n1       ID_KGE2HY\n2       ID_VHV9BL\n3       ID_ZT0VEJ\n4       ID_5NFXVY\n          ...    \n7142    ID_7RINJF\n7143    ID_UAYBOC\n7144    ID_M0YJD0\n7145    ID_PI0GHW\n7146    ID_VNBDTG\nName: ID, Length: 7147, dtype: object"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"def build_tf_dataset(npy_dir_path, label_df=None, batch_size=32, shuffle=True):\n    npy_dir = Path(npy_dir_path)\n\n    # Step 1: List all .npy files\n    file_paths = list(npy_dir.glob(\"*.npy\"))\n\n    # Step 2: If labels are available, map IDs to labels\n    if label_df is not None:\n        label_df[\"ID\"] = label_df[\"ID\"].astype(str)\n        label_map = dict(zip(label_df[\"ID\"], label_df[\"label\"]))\n\n        # Build (file_path, label) pairs\n        data = []\n        for file in file_paths:\n            file_id = file.stem  # e.g. '1234' from '1234.npy'\n            if file_id in label_map:\n                label = label_map[file_id]\n                data.append((str(file), label))\n\n        file_paths, labels = zip(*data)\n    else:\n        # Just raw file paths for unlabeled data (e.g. test set)\n        file_paths = [str(f) for f in file_paths]\n        labels = None\n\n    # Step 3: Define how to load one sample\n    def load_npy(file_path, label=None):\n        def _load(path):\n            arr = np.load(path.numpy().decode(\"utf-8\"))\n            return arr.astype(np.float32)\n\n        image = tf.py_function(_load, [file_path], tf.float32)\n        image.set_shape([64, 64, 12]) \n\n        if label is not None:\n            return image, label\n        else:\n            return image\n\n    # Step 4: Build tf.data.Dataset\n    if labels is not None:\n        ds = tf.data.Dataset.from_tensor_slices((list(file_paths), list(labels)))\n        ds = ds.map(load_npy, num_parallel_calls=tf.data.AUTOTUNE)\n    else:\n        ds = tf.data.Dataset.from_tensor_slices(list(file_paths))\n        ds = ds.map(lambda x: load_npy(x, None), num_parallel_calls=tf.data.AUTOTUNE)\n\n    if shuffle and labels is not None:\n        ds = ds.shuffle(buffer_size=len(file_paths))\n\n    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\n    return ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T12:24:45.468937Z","iopub.execute_input":"2025-07-24T12:24:45.469153Z","iopub.status.idle":"2025-07-24T12:24:45.487076Z","shell.execute_reply.started":"2025-07-24T12:24:45.469135Z","shell.execute_reply":"2025-07-24T12:24:45.486459Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"train_dataset = build_tf_dataset(train_data_path, train_csv)\ntest_dataset = build_tf_dataset(test_data_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-24T12:24:45.487809Z","iopub.execute_input":"2025-07-24T12:24:45.488047Z","iopub.status.idle":"2025-07-24T12:24:46.701628Z","shell.execute_reply.started":"2025-07-24T12:24:45.488024Z","shell.execute_reply":"2025-07-24T12:24:46.700837Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1753359886.497097      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}